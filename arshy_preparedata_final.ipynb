{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arshy_preparedata_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will see how we can preprocess the data that was downloaded from Open Neuro. \n",
        "\n",
        "The preproces we used \n",
        "- `filter` to filter the signals between desired Hz\n",
        "- `resample` to resample the eeg signal from acqusition frequency to a desired frequency\n",
        "- `ICA` to remove `ecg, eog` related artifacts \n",
        "- `fized length epochs` to break the continuous signal to number of samples"
      ],
      "metadata": {
        "id": "vLdQF_bMxiAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following approaches were used to preproces that data\n",
        "\n",
        "Approach 1\n",
        "\n",
        "- filter between 1 and 40\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA\n",
        "- epoch 50s\n",
        "\n",
        "Approach 2\n",
        "\n",
        "- filter between 1 and 40\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA\n",
        "- epoch 50s and average\n",
        "\n",
        "Approach 3\n",
        "\n",
        "- filter between 1 and 20\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA on epochs\n",
        "- epoch 50s\n",
        "\n",
        "Approach 4\n",
        "\n",
        "- filter between 1 and 20\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA on epochs\n",
        "- epoch 50s and average\n"
      ],
      "metadata": {
        "id": "_CdVHs4ugLLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastcore mne[data] mne-bids PyQt5 -Uqq"
      ],
      "metadata": {
        "id": "eZ9teOhqog_x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlE0mMdeYRwe",
        "outputId": "43b1c032-a3ea-4158-8a2a-fb1216024402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/colab_notebooks/algovera/lynxhack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvqi41tWYXns",
        "outputId": "ff6c0604-8c17-495c-f165-7141b0c17b03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab_notebooks/algovera/lynxhack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib\n",
        "from pathlib import Path\n",
        "import mne\n",
        "import mne_bids\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from fastcore.parallel import parallel\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "metadata": {
        "id": "RQqpi4AGYkYO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin,BaseEstimator\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ZV4m9Rr3veqA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_approach12(fn):\n",
        "    '''\n",
        "    approach1\n",
        "    filter - 1 and 40\n",
        "    resample - 100hz\n",
        "    ica - 20 components\n",
        "    epoch - 50s\n",
        "\n",
        "    approach2\n",
        "    same but average all epochs to one   \n",
        "    '''\n",
        "    path = f\"processeddata/individuals/afterica\"\n",
        "    sub = str(fn).split('/')[-1].split('_')[0]\n",
        "    session = str(fn).split('/')[-1].split('_')[1]\n",
        "    label = str(fn).split('/')[-1].split('_')[2]\n",
        "\n",
        "    raw = mne.io.read_raw_brainvision(fn, preload=True)\n",
        "    raw = raw.resample(100).filter(l_freq=1, h_freq=40)\n",
        "\n",
        "    ica = mne.preprocessing.ICA(n_components=20, \n",
        "                                random_state=0)\n",
        "    ica.fit(raw)\n",
        "\n",
        "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
        "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
        "    \n",
        "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
        "\n",
        "    raw_after = ica.apply(raw, \n",
        "                          exclude=ica.exclude)\n",
        "\n",
        "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
        "                                                duration=50,  \n",
        "                                                overlap=0)\n",
        "\n",
        "    fn1 = f\"{path}/{label}_{sub}_{session}_approach1.npy\"\n",
        "    fn2 = f\"{path}/{label}_{sub}_{session}_approach2.npy\"\n",
        "\n",
        "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
        "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
      ],
      "metadata": {
        "id": "aCDNfYWkrjd9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_approach34(fn):\n",
        "    '''\n",
        "    approach1\n",
        "    filter - 1 and 20\n",
        "    resample - 100hz\n",
        "    ica - 20 components\n",
        "    epoch - 50s\n",
        "\n",
        "    approach2\n",
        "    same but average all epochs to one   \n",
        "    '''\n",
        "    path = f\"processeddata/individuals/afterica\"\n",
        "    sub = str(fn).split('/')[-1].split('_')[0]\n",
        "    session = str(fn).split('/')[-1].split('_')[1]\n",
        "    label = str(fn).split('/')[-1].split('_')[2]\n",
        "\n",
        "    raw = mne.io.read_raw_brainvision(fn, preload=True)\n",
        "    raw = raw.resample(100).filter(l_freq=1, h_freq=20)\n",
        "\n",
        "    ica = mne.preprocessing.ICA(n_components=20, \n",
        "                                random_state=0)\n",
        "    ica.fit(raw)\n",
        "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
        "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
        "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
        "\n",
        "    raw_after = ica.apply(raw, \n",
        "                          exclude=ica.exclude)\n",
        "\n",
        "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
        "                                                duration=50,  \n",
        "                                                overlap=0)\n",
        "\n",
        "    fn1 = f\"{path}/{label}_{sub}_{session}_approach3.npy\"\n",
        "    fn2 = f\"{path}/{label}_{sub}_{session}_approach4.npy\"\n",
        "\n",
        "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
        "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
      ],
      "metadata": {
        "id": "sEZc7zM9hp0s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = []\n",
        "for path in Path('hackdataset').rglob('*.vhdr'):\n",
        "    filenames.append(path)"
      ],
      "metadata": {
        "id": "ilub4vcmuj6x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "parallel(prepare_approach12, \n",
        "         filenames, \n",
        "         n_workers=8, \n",
        "         progress=True)"
      ],
      "metadata": {
        "id": "DMYn6EZAvvIl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "parallel(prepare_approach34, \n",
        "         filenames, \n",
        "         n_workers=8, \n",
        "         progress=True)"
      ],
      "metadata": {
        "id": "4v5rjatSv7np"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare df and Standard Scalar"
      ],
      "metadata": {
        "id": "8I4kFn-rONB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SScaler3D(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        self.scaler.fit(X.reshape(X.shape[0], -1))\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        return self.scaler.transform(X.reshape(X.shape[0], -1)).reshape(X.shape)"
      ],
      "metadata": {
        "id": "cCpw_3huvhf5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_df_ss(approach):\n",
        "    fns = glob(f'processeddata/individuals/afterica/*{approach}.npy')\n",
        "\n",
        "    fns_2 = []\n",
        "    label = []\n",
        "    subject = []\n",
        "    session = []\n",
        "    for fn in fns:\n",
        "        fns_2.append(fn)\n",
        "        label.append(str(fn).split('/')[-1].split('_')[0])\n",
        "        subject.append(str(fn).split('/')[-1].split('_')[1])\n",
        "        session.append(str(fn).split('/')[-1].split('_')[2])\n",
        "\n",
        "    df = pd.DataFrame([fns_2, label, subject, session]).T\n",
        "    df.columns = ['fns', 'label', 'subject', 'session']\n",
        "\n",
        "    val_subs = ['sub-51', 'sub-52', 'sub-53', 'sub-54', 'sub-55', 'sub-56', 'sub-57', 'sub-58', 'sub-59', 'sub-60']\n",
        "    df['is_valid'] = False\n",
        "    df.loc[df[df['subject'].isin(val_subs)].index, 'is_valid'] = True\n",
        "\n",
        "    df.to_csv(f'{approach}infos.csv', index=False)\n",
        "\n",
        "    if approach in ['approach1', 'approach3']:\n",
        "        features = []\n",
        "        for fn in df[df['is_valid']==False].fns.values:\n",
        "            temp = np.load(fn)\n",
        "            if temp.shape[0] == 6:\n",
        "                features.append(temp)\n",
        "        \n",
        "        features = np.vstack(features)\n",
        "        ss = SScaler3D()\n",
        "        ss.fit(features[:int(0.5*features.shape[0])])\n",
        "\n",
        "        pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))\n",
        "\n",
        "    else:\n",
        "        features = []\n",
        "        for fn in df[df['is_valid']==False].fns.values:\n",
        "            temp = np.load(fn)\n",
        "            features.append(temp)\n",
        "        \n",
        "        features = np.vstack(features)\n",
        "        ss = StandardScaler()\n",
        "        ss.fit(features[:int(0.5*features.shape[0])])\n",
        "\n",
        "        pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "EpFQzzmnOMQa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_df_ss('approach1')"
      ],
      "metadata": {
        "id": "XCcdYsFAxAFM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_df_ss('approach2')"
      ],
      "metadata": {
        "id": "y0OSyDCNxHoL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_df_ss('approach3')"
      ],
      "metadata": {
        "id": "v068lSxFxKnB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_df_ss('approach4')"
      ],
      "metadata": {
        "id": "t1dLkdE5xLmI"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}