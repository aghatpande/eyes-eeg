{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLdQF_bMxiAM"
   },
   "source": [
    "In this notebook, we will see how we can preprocess the data that was downloaded from Open Neuro. \n",
    "\n",
    "The preproces we used \n",
    "- `filter` to filter the signals between desired Hz\n",
    "- `resample` to resample the eeg signal from acqusition frequency to a desired frequency\n",
    "- `ICA` to remove `ecg, eog` related artifacts \n",
    "- `fized length epochs` to break the continuous signal to number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CdVHs4ugLLV"
   },
   "source": [
    "The following approaches were used to preproces that data\n",
    "\n",
    "Approach 1\n",
    "\n",
    "- filter between 1 and 40\n",
    "- resample from 500hz to 100 hz\n",
    "- remove artifacts based on ICA\n",
    "- epoch 50s\n",
    "\n",
    "Approach 2\n",
    "\n",
    "- filter between 1 and 40\n",
    "- resample from 500hz to 100 hz\n",
    "- remove artifacts based on ICA\n",
    "- epoch 50s and average\n",
    "\n",
    "Approach 3\n",
    "\n",
    "- filter between 1 and 20\n",
    "- resample from 500hz to 100 hz\n",
    "- remove artifacts based on ICA on epochs\n",
    "- epoch 50s\n",
    "\n",
    "Approach 4\n",
    "\n",
    "- filter between 1 and 20\n",
    "- resample from 500hz to 100 hz\n",
    "- remove artifacts based on ICA on epochs\n",
    "- epoch 50s and average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ9teOhqog_x"
   },
   "outputs": [],
   "source": [
    "!pip install fastcore mne[data] mne-bids PyQt5 -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlE0mMdeYRwe",
    "outputId": "43b1c032-a3ea-4158-8a2a-fb1216024402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvqi41tWYXns",
    "outputId": "ff6c0604-8c17-495c-f165-7141b0c17b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/colab_notebooks/algovera/lynxhack\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/colab_notebooks/algovera/lynxhack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQqpi4AGYkYO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "import mne\n",
    "import mne_bids\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from fastcore.parallel import parallel\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV4m9Rr3veqA"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCDNfYWkrjd9"
   },
   "outputs": [],
   "source": [
    "def prepare_approach12(fn):\n",
    "    '''\n",
    "    approach1\n",
    "    filter - 1 and 40\n",
    "    resample - 100hz\n",
    "    ica - 20 components\n",
    "    epoch - 50s\n",
    "\n",
    "    approach2\n",
    "    same but average all epochs to one   \n",
    "    '''\n",
    "    path = f\"processeddata/individuals/afterica\"\n",
    "    sub = str(fn).split('/')[-1].split('_')[0]\n",
    "    session = str(fn).split('/')[-1].split('_')[1]\n",
    "    label = str(fn).split('/')[-1].split('_')[2]\n",
    "\n",
    "    raw = mne.io.read_raw_brainvision(fn, preload=True)\n",
    "    raw = raw.resample(100).filter(l_freq=1, h_freq=40)\n",
    "\n",
    "    ica = mne.preprocessing.ICA(n_components=20, \n",
    "                                random_state=0)\n",
    "    ica.fit(raw)\n",
    "\n",
    "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
    "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
    "    \n",
    "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
    "\n",
    "    raw_after = ica.apply(raw, \n",
    "                          exclude=ica.exclude)\n",
    "\n",
    "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
    "                                                duration=50,  \n",
    "                                                overlap=0)\n",
    "\n",
    "    fn1 = f\"{path}/{label}_{sub}_{session}_approach1.npy\"\n",
    "    fn2 = f\"{path}/{label}_{sub}_{session}_approach2.npy\"\n",
    "\n",
    "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
    "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEZc7zM9hp0s"
   },
   "outputs": [],
   "source": [
    "def prepare_approach34(fn):\n",
    "    '''\n",
    "    approach1\n",
    "    filter - 1 and 20\n",
    "    resample - 100hz\n",
    "    ica - 20 components\n",
    "    epoch - 50s\n",
    "\n",
    "    approach2\n",
    "    same but average all epochs to one   \n",
    "    '''\n",
    "    path = f\"processeddata/individuals/afterica\"\n",
    "    sub = str(fn).split('/')[-1].split('_')[0]\n",
    "    session = str(fn).split('/')[-1].split('_')[1]\n",
    "    label = str(fn).split('/')[-1].split('_')[2]\n",
    "\n",
    "    raw = mne.io.read_raw_brainvision(fn, preload=True)\n",
    "    raw = raw.resample(100).filter(l_freq=1, h_freq=20)\n",
    "\n",
    "    ica = mne.preprocessing.ICA(n_components=20, \n",
    "                                random_state=0)\n",
    "    ica.fit(raw)\n",
    "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
    "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
    "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
    "\n",
    "    raw_after = ica.apply(raw, \n",
    "                          exclude=ica.exclude)\n",
    "\n",
    "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
    "                                                duration=50,  \n",
    "                                                overlap=0)\n",
    "\n",
    "    fn1 = f\"{path}/{label}_{sub}_{session}_approach3.npy\"\n",
    "    fn2 = f\"{path}/{label}_{sub}_{session}_approach4.npy\"\n",
    "\n",
    "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
    "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilub4vcmuj6x"
   },
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for path in Path('hackdataset').rglob('*.vhdr'):\n",
    "    filenames.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMYn6EZAvvIl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "parallel(prepare_approach12, \n",
    "         filenames, \n",
    "         n_workers=8, \n",
    "         progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4v5rjatSv7np"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "parallel(prepare_approach34, \n",
    "         filenames, \n",
    "         n_workers=8, \n",
    "         progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I4kFn-rONB_"
   },
   "source": [
    "# Prepare df and Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCpw_3huvhf5"
   },
   "outputs": [],
   "source": [
    "class SScaler3D(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(X.shape[0], -1))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape(X.shape[0], -1)).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpFQzzmnOMQa"
   },
   "outputs": [],
   "source": [
    "def prepare_df_ss(approach):\n",
    "    fns = glob(f'processeddata/individuals/afterica/*{approach}.npy')\n",
    "\n",
    "    fns_2 = []\n",
    "    label = []\n",
    "    subject = []\n",
    "    session = []\n",
    "    for fn in fns:\n",
    "        fns_2.append(fn)\n",
    "        label.append(str(fn).split('/')[-1].split('_')[0])\n",
    "        subject.append(str(fn).split('/')[-1].split('_')[1])\n",
    "        session.append(str(fn).split('/')[-1].split('_')[2])\n",
    "\n",
    "    df = pd.DataFrame([fns_2, label, subject, session]).T\n",
    "    df.columns = ['fns', 'label', 'subject', 'session']\n",
    "\n",
    "    val_subs = ['sub-51', 'sub-52', 'sub-53', 'sub-54', 'sub-55', 'sub-56', 'sub-57', 'sub-58', 'sub-59', 'sub-60']\n",
    "    df['is_valid'] = False\n",
    "    df.loc[df[df['subject'].isin(val_subs)].index, 'is_valid'] = True\n",
    "\n",
    "    df.to_csv(f'{approach}infos.csv', index=False)\n",
    "\n",
    "    if approach in ['approach1', 'approach3']:\n",
    "        features = []\n",
    "        for fn in df[df['is_valid']==False].fns.values:\n",
    "            temp = np.load(fn)\n",
    "            if temp.shape[0] == 6:\n",
    "                features.append(temp)\n",
    "        \n",
    "        features = np.vstack(features)\n",
    "        ss = SScaler3D()\n",
    "        ss.fit(features[:int(0.5*features.shape[0])])\n",
    "\n",
    "        pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))\n",
    "\n",
    "    else:\n",
    "        features = []\n",
    "        for fn in df[df['is_valid']==False].fns.values:\n",
    "            temp = np.load(fn)\n",
    "            features.append(temp)\n",
    "        \n",
    "        features = np.vstack(features)\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(features[:int(0.5*features.shape[0])])\n",
    "\n",
    "        pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCcdYsFAxAFM"
   },
   "outputs": [],
   "source": [
    "prepare_df_ss('approach1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0OSyDCNxHoL"
   },
   "outputs": [],
   "source": [
    "prepare_df_ss('approach2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v068lSxFxKnB"
   },
   "outputs": [],
   "source": [
    "prepare_df_ss('approach3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1dLkdE5xLmI"
   },
   "outputs": [],
   "source": [
    "prepare_df_ss('approach4')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
